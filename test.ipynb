{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:14:28.157731Z",
     "start_time": "2024-08-06T16:14:28.132808Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f1daea1655f0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:10:54.753269Z",
     "start_time": "2024-08-06T16:10:54.721042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.models.vision_transformer import VisionTransformer\n",
    "import torch\n",
    "\n",
    "model = VisionTransformer(img_size=32, patch_size=4, num_classes=0, embed_dim=128, depth=4, num_heads=8, mlp_ratio=4)\n",
    "#print(model)\n",
    "\n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87333db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 128])\n"
     ]
    }
   ],
   "source": [
    "from timm.layers.patch_dropout import PatchDropout\n",
    "from vitssm.models.modules import LearnablePositionalEncoding\n",
    "import torch\n",
    "\n",
    "drop = PatchDropout(num_prefix_tokens=0)\n",
    "pos_enc = LearnablePositionalEncoding(16**2, 128, p_dropout=0.5)\n",
    "\n",
    "x = torch.ones((32, 32**2, 128))\n",
    "\n",
    "x = pos_enc(x)\n",
    "x = drop(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2097ec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(x // 16 for x in (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5f72e35eb0fecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T16:00:40.422223Z",
     "start_time": "2024-08-06T16:00:40.415757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): MultiHeadDispatch(\n",
       "    (attention): LocalAttention(\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (in_proj_container): InputProjection(\n",
       "      (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (1): MultiHeadDispatch(\n",
       "    (attention): LocalAttention(\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (in_proj_container): InputProjection(\n",
       "      (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (2): MultiHeadDispatch(\n",
       "    (attention): LocalAttention(\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (in_proj_container): InputProjection(\n",
       "      (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (3): MultiHeadDispatch(\n",
       "    (attention): LocalAttention(\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (in_proj_container): InputProjection(\n",
       "      (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xformers.components.multi_head_dispatch import MultiHeadDispatch\n",
    "from xformers.components.attention import ScaledDotProduct, LocalAttention\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#att = ScaledDotProduct(causal=True, seq_len=30)\n",
    "att = LocalAttention(causal=True, window_size=5)\n",
    "blocks = nn.Sequential(*(MultiHeadDispatch(128, 4, att) for _ in range(4)))\n",
    "\n",
    "x = torch.randn(32, 30, 128)\n",
    "\n",
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd1d26c9913d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T15:02:49.026402Z",
     "start_time": "2024-08-06T15:02:48.996449Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [32, 30] but got: [32, 40].",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m v \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones((\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m40\u001B[39m, \u001B[38;5;241m128\u001B[39m))\n\u001B[1;32m     14\u001B[0m mask \u001B[38;5;241m=\u001B[39m LowerTriangularMask()\n\u001B[0;32m---> 16\u001B[0m \u001B[43matt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/xformers/components/attention/scaled_dot_product.py:131\u001B[0m, in \u001B[0;36mScaledDotProduct.forward\u001B[0;34m(self, q, k, v, att_mask, *args, **kwargs)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;66;03m# Attend: (B x nh, S, hs) x (B x nh, hs, S) -> (B x nh, S, S)\u001B[39;00m\n\u001B[0;32m--> 131\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43mq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43matt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43matt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn_drop\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/xformers/components/attention/core.py:331\u001B[0m, in \u001B[0;36mscaled_dot_product_attention\u001B[0;34m(q, k, v, att_mask, dropout, block_size)\u001B[0m\n\u001B[1;32m    327\u001B[0m     att \u001B[38;5;241m=\u001B[39m _apply_dropout(att, dropout)\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;66;03m# Get to the predicted values, for all heads\u001B[39;00m\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;66;03m# y = att @ v  # (N, S, S) x (N, S, hs) -> (N, S, hs)\u001B[39;00m\n\u001B[0;32m--> 331\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mbmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43matt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/xformers/components/attention/core.py:168\u001B[0m, in \u001B[0;36mbmm\u001B[0;34m(a, b)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m a\u001B[38;5;241m.\u001B[39mis_sparse:\n\u001B[1;32m    167\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _sparse_bmm(a, b)\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected size for first two dimensions of batch2 tensor to be: [32, 30] but got: [32, 40]."
     ]
    }
   ],
   "source": [
    "from xformers.components.attention import ScaledDotProduct\n",
    "from xformers.ops.fmha.attn_bias import LowerTriangularMask\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "att = ScaledDotProduct(seq_len=30, to_seq_len=40)\n",
    "q_proj = nn.Linear(128, 128)\n",
    "k_proj = nn.Linear(128, 128)\n",
    "v_proj = nn.Linear(128, 128)\n",
    "\n",
    "q = torch.ones((32, 30, 128))\n",
    "k = torch.ones((32, 30, 128))\n",
    "v = torch.ones((32, 40, 128))\n",
    "mask = LowerTriangularMask()\n",
    "\n",
    "att(q_proj(q), k_proj(k), v_proj(v), mask=mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aee116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([960, 3, 32, 32])\n",
      "torch.Size([960, 128, 8, 8])\n",
      "torch.Size([960, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "from vitssm.models.next_frame_prediction import LatentNextFramePrediction\n",
    "from xformers.components.positional_embedding import SinePositionalEmbedding\n",
    "import xformers\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "patchify = nn.Conv2d(3, 128, kernel_size=4, stride=4)\n",
    "pos_enc = SinePositionalEmbedding(128)\n",
    "\n",
    "x = torch.zeros((32, 30, 32, 32, 3))\n",
    "b, t, h, w, c = x.shape\n",
    "x = rearrange(x, \"b t h w c -> (b t) c h w\")\n",
    "print(x.shape)\n",
    "x = patchify(x)\n",
    "print(x.shape)\n",
    "x = rearrange(x, \"(b t) e h w -> (b t) (h w) e\", b=b, t=t)\n",
    "print(x.shape)\n",
    "x = pos_enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbe1baed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T05:07:04.092454Z",
     "start_time": "2024-08-07T05:07:04.065846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "from xformers.components.positional_embedding import SinePositionalEmbedding\n",
    "from einops import rearrange\n",
    "\n",
    "pos_enc = SinePositionalEmbedding(3)\n",
    "\n",
    "x1 = torch.zeros((1, 1, 4, 4, 3))\n",
    "x2 = torch.zeros((1, 1, 8, 8, 3))\n",
    "b1, t1, h1, w1, c1 = x1.shape\n",
    "b2, t2, h2, w2, c2 = x2.shape\n",
    "\n",
    "x1 = rearrange(x1, \"b t h w c -> b (t h w) c\")\n",
    "x1 = pos_enc(x1)\n",
    "#x1 = rearrange(x1, \"b (t h w) c -> (b t) h w c\", b=b1, t=t1, h=h1, w=w1)\n",
    "\n",
    "x2 = rearrange(x2, \"b t h w c -> b (t h w) c\")\n",
    "x2 = pos_enc(x2)\n",
    "#x2 = rearrange(x2, \"b (t h w) c -> (b t) h w c\", b=b2, t=t2, h=h2, w=w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bcc8165a63cb0864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:33:58.107380Z",
     "start_time": "2024-08-29T14:33:46.791283Z"
    }
   },
   "source": [
    "from vitssm.models.next_frame_prediction import LatentNextFramePrediction\n",
    "import torch\n",
    "\n",
    "model = LatentNextFramePrediction(frame_in_size=(32, 32), frame_out_size=(32, 32), patch_size=4, latent_dim=32, n_heads=2, n_frames=10)\n",
    "x = torch.randn(32, 10, 32, 32, 3)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/micromamba/envs/ViTSSM/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/david/micromamba/envs/ViTSSM/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: triton is not available\n",
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n",
      "/home/david/micromamba/envs/ViTSSM/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 1024, 32]) torch.Size([320, 64, 32]) torch.Size([320, 1, 32])\n",
      "torch.Size([320, 1024, 32]) torch.Size([320, 64, 32]) torch.Size([320, 1, 32])\n",
      "torch.Size([320, 1024, 32]) torch.Size([320, 64, 32]) torch.Size([320, 1, 32])\n",
      "torch.Size([320, 1024, 32]) torch.Size([320, 64, 32]) torch.Size([320, 1, 32])\n",
      "torch.Size([32, 10, 32, 32, 32])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:54:22.193841Z",
     "start_time": "2024-08-29T14:54:22.011755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from einops import rearrange\n",
    "\n",
    "x_next_frame = torch.randn(9, 4*4, 32*32*3)\n",
    "t, h, w = 3, 32, 32\n",
    "\n",
    "x_next_frame = rearrange(\n",
    "    x_next_frame,\n",
    "    \"(b t) (hp wp) (h w c) -> b t h w c\",\n",
    "    t=t,\n",
    "    h=h,\n",
    "    w=w,\n",
    ")\n",
    "print(x_next_frame.shape)"
   ],
   "id": "b0f79a06e5636b4b",
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"(b t) (hp wp) (h w c) -> b t h w c\".\n Input tensor shape: torch.Size([9, 16, 3072]). Additional info: {'t': 3, 'h': 32, 'w': 32}.\n Identifiers only on one side of expression (should be on both): {'hp', 'wp'}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/einops/einops.py:522\u001B[0m, in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    521\u001B[0m shape \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mshape(tensor)\n\u001B[0;32m--> 522\u001B[0m recipe \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_transformation_recipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43maxes_lengths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _apply_recipe(\n\u001B[1;32m    524\u001B[0m     backend, recipe, cast(Tensor, tensor), reduction_type\u001B[38;5;241m=\u001B[39mreduction, axes_lengths\u001B[38;5;241m=\u001B[39mhashable_axes_lengths\n\u001B[1;32m    525\u001B[0m )\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/einops/einops.py:312\u001B[0m, in \u001B[0;36m_prepare_transformation_recipe\u001B[0;34m(pattern, operation, axes_names, ndim)\u001B[0m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(difference) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 312\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m EinopsError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIdentifiers only on one side of expression (should be on both): \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(difference))\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m operation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepeat\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mEinopsError\u001B[0m: Identifiers only on one side of expression (should be on both): {'hp', 'wp'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m x_next_frame \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m9\u001B[39m, \u001B[38;5;241m4\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m32\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m32\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m      4\u001B[0m t, h, w \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m32\u001B[39m\n\u001B[0;32m----> 6\u001B[0m x_next_frame \u001B[38;5;241m=\u001B[39m \u001B[43mrearrange\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_next_frame\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m(b t) (hp wp) (h w c) -> b t h w c\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(x_next_frame\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/einops/einops.py:591\u001B[0m, in \u001B[0;36mrearrange\u001B[0;34m(tensor, pattern, **axes_lengths)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrearrange\u001B[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39maxes_lengths) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    537\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001B[39;00m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    589\u001B[0m \n\u001B[1;32m    590\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrearrange\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43maxes_lengths\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/micromamba/envs/ViTSSM/lib/python3.12/site-packages/einops/einops.py:533\u001B[0m, in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    531\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m Input is list. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    532\u001B[0m message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdditional info: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(axes_lengths)\n\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m EinopsError(message \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(e))\n",
      "\u001B[0;31mEinopsError\u001B[0m:  Error while processing rearrange-reduction pattern \"(b t) (hp wp) (h w c) -> b t h w c\".\n Input tensor shape: torch.Size([9, 16, 3072]). Additional info: {'t': 3, 'h': 32, 'w': 32}.\n Identifiers only on one side of expression (should be on both): {'hp', 'wp'}"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T14:36:07.529893Z",
     "start_time": "2024-08-26T14:36:07.498967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "nn.activation"
   ],
   "id": "5908edd7edcea7ef",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'activation'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m----> 2\u001B[0m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactivation\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch.nn' has no attribute 'activation'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428d2302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.966629547095765"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb9250dc232ff44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T16:48:07.995713Z",
     "start_time": "2024-07-22T16:48:07.929197Z"
    }
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "\n",
    "base_config = OmegaConf.load(Path(\"run_configs\") / \"base_config.yml\")\n",
    "config = OmegaConf.load(Path(\"run_configs\") / \"config_run_1.yml\")\n",
    "conf = OmegaConf.merge(base_config, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98791ad321ba8bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T04:56:58.895687Z",
     "start_time": "2024-07-30T04:56:50.901577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8),\n",
       " tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import MovingMNIST\n",
    "from vitssm.data.datasets import NextFrameDataset\n",
    "\n",
    "\n",
    "mnist = MovingMNIST(root=\"data\", split=\"train\", split_ratio=10, download=False)\n",
    "data = NextFrameDataset(mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c60daced68ba04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T04:57:37.194309Z",
     "start_time": "2024-07-30T04:57:37.184433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1, 64, 64]) torch.Size([9, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape, data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487aa3bdbd767cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T16:48:08.019094Z",
     "start_time": "2024-07-22T16:48:08.008539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xformers\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b2d6e77dc7f912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T16:48:08.930310Z",
     "start_time": "2024-07-22T16:48:08.019842Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'addict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mvitssm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maction_recognition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ActionRecognitionEngine\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mhub\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpytorch/vision:v0.10.0\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malexnet\u001B[39m\u001B[38;5;124m'\u001B[39m, pretrained\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/mnt/c/Users/david/Documents/Uni/vitssm/vitssm/engine/__init__.py:15\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m set_seeds\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_metric_container\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_dataloaders\n\u001B[1;32m     17\u001B[0m wandb\u001B[38;5;241m.\u001B[39mlogin()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mModelEngine\u001B[39;00m:\n",
      "File \u001B[0;32m/mnt/c/Users/david/Documents/Uni/vitssm/vitssm/data/__init__.py:6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maddict\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dict\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader, random_split\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Compose, Resize, ToImage, ToDtype, Normalize\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'addict'"
     ]
    }
   ],
   "source": [
    "from vitssm.engine.tasks import ActionRecognitionEngine\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "engine = ActionRecognitionEngine(model, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a01979edfaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vitssm.utils.metrics import MetricContainer\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "import torch\n",
    "\n",
    "x = torch.randn(100, 100)\n",
    "y = torch.randint(0, 2, (100, 100))\n",
    "metric_col(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d77fd87f203b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vitssm.engine import build_optimizer\n",
    "from addict import Dict\n",
    "from torch import nn\n",
    "\n",
    "config = Dict({\n",
    "    \"optimization\": {\n",
    "        \"optimizer\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"args\": {\n",
    "                \"lr\": 0.001,\n",
    "                \"weight_decay\": 0.0\n",
    "            }\n",
    "        }\n",
    "    }    \n",
    "})\n",
    "\n",
    "model = nn.Linear(10, 10)\n",
    "build_optimizer(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579710ed660610d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb498001513a97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vitssm.data import build_dataset\n",
    "from addict import Dict\n",
    "import kappaconfig as kc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "config = kc.from_file_uri(Path(\"run_configs\") / \"base_config.yml\")\n",
    "print(config.keys())\n",
    "res = kc.DefaultResolver()\n",
    "config = Dict(res.resolve(config))\n",
    "\n",
    "dataset = build_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972fac984f68fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1000][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f623708d4b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0ac6bd1ed53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0a9cf007aff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].permute(0, 2, 3, 1).repeat(1, 1, 1, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb78a7e053d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import write_video\n",
    "\n",
    "write_video(\"test.mp4\", train_dataset[0].permute(0, 2, 3, 1).repeat(1, 1, 1, 3), fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
